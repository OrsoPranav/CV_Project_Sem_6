{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-10T12:15:22.997689Z",
     "iopub.status.busy": "2025-03-10T12:15:22.997485Z",
     "iopub.status.idle": "2025-03-10T12:15:37.214972Z",
     "shell.execute_reply": "2025-03-10T12:15:37.214053Z",
     "shell.execute_reply.started": "2025-03-10T12:15:22.997669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:17:29.792902Z",
     "iopub.status.busy": "2025-03-10T12:17:29.792595Z",
     "iopub.status.idle": "2025-03-10T12:17:29.800973Z",
     "shell.execute_reply": "2025-03-10T12:17:29.800004Z",
     "shell.execute_reply.started": "2025-03-10T12:17:29.792878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Ensure reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:17:16.476474Z",
     "iopub.status.busy": "2025-03-10T12:17:16.476008Z",
     "iopub.status.idle": "2025-03-10T12:17:16.629145Z",
     "shell.execute_reply": "2025-03-10T12:17:16.628323Z",
     "shell.execute_reply.started": "2025-03-10T12:17:16.476451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Set up the device and paths - Optimized for Kaggle T4 x2 GPUs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "    # Print GPU information\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB\")\n",
    "        print(f\"Memory Reserved: {torch.cuda.memory_reserved(i) / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "\n",
    "\n",
    "# Manually set paths for ISIC 2017 dataset on Kaggle\n",
    "base_dir = \"/kaggle/input/isic-2017/\"  # Adjust this based on your dataset's actual name\n",
    "train_dir = os.path.join(base_dir, \"/kaggle/input/isic2017/ISIC-2017_Training_Data/ISIC-2017_Training_Data\")\n",
    "train_gt_dir = os.path.join(base_dir, \"/kaggle/input/isic2017/ISIC-2017_Training_Part1_GroundTruth/ISIC-2017_Training_Part1_GroundTruth\")\n",
    "valid_dir = os.path.join(base_dir, \"/kaggle/input/isic2017/ISIC-2017_Validation_Data/ISIC-2017_Validation_Data\")\n",
    "valid_gt_dir = os.path.join(base_dir, \"/kaggle/input/isic2017/ISIC-2017_Validation_Part1_GroundTruth/ISIC-2017_Validation_Part1_GroundTruth\")\n",
    "test_dir = os.path.join(base_dir, \"/kaggle/input/isic2017/ISIC-2017_Test_v2_Data/ISIC-2017_Test_v2_Data\")\n",
    "test_gt_dir = os.path.join(base_dir, \"/kaggle/input/isic2017/ISIC-2017_Test_v2_Part1_GroundTruth/ISIC-2017_Test_v2_Part1_GroundTruth\")\n",
    "# Print paths to verify\n",
    "print(f\"Training data path: {train_dir}\")\n",
    "print(f\"Training GT path: {train_gt_dir}\")\n",
    "print(f\"Validation data path: {valid_dir}\")\n",
    "print(f\"Validation GT path: {valid_gt_dir}\")\n",
    "print(f\"Test data path: {test_dir}\")\n",
    "print(f\"Test GT path: {test_gt_dir}\")\n",
    "\n",
    "# Check if directories exist\n",
    "for dir_path in [train_dir, train_gt_dir, valid_dir, valid_gt_dir, test_dir, test_gt_dir]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        print(f\"WARNING: Directory does not exist: {dir_path}\")\n",
    "    else:\n",
    "        print(f\"Directory found: {dir_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:17:52.030002Z",
     "iopub.status.busy": "2025-03-10T12:17:52.029666Z",
     "iopub.status.idle": "2025-03-10T12:17:52.037124Z",
     "shell.execute_reply": "2025-03-10T12:17:52.036404Z",
     "shell.execute_reply.started": "2025-03-10T12:17:52.029973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Create a custom dataset class for ISIC2017\n",
    "class ISIC2017Dataset(Dataset):\n",
    "    def __init__(self, data_dir, gt_dir, transform=None, return_name=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Directory with all the images\n",
    "            gt_dir: Directory with ground truth masks\n",
    "            transform: Optional transform to be applied to the images\n",
    "            return_name: Whether to return the image filename\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.return_name = return_name\n",
    "        \n",
    "        # Get list of all image files\n",
    "        self.image_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.jpg')])\n",
    "        \n",
    "        # Extract label from ground truth files (for classification task)\n",
    "        # Note: For ISIC 2017, we'll convert the segmentation masks to classification labels\n",
    "        # (presence of lesion or not) for simplicity\n",
    "        self.labels = []\n",
    "        for img_file in self.image_files:\n",
    "            # Get corresponding mask filename (ISIC2017 naming convention)\n",
    "            img_id = img_file.split('.')[0]\n",
    "            mask_file = f\"{img_id}_segmentation.png\"\n",
    "            \n",
    "            if os.path.exists(os.path.join(gt_dir, mask_file)):\n",
    "                # Check if the mask has any positive pixels (lesion present)\n",
    "                mask = Image.open(os.path.join(gt_dir, mask_file)).convert(\"L\")\n",
    "                mask_np = np.array(mask)\n",
    "                # If any pixel is > 0, consider it as positive class (1), else negative (0)\n",
    "                has_lesion = 1 if np.sum(mask_np > 0) > 0 else 0\n",
    "                self.labels.append(has_lesion)\n",
    "            else:\n",
    "                # If no mask exists, assume no lesion (class 0)\n",
    "                self.labels.append(0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.return_name:\n",
    "            return image, label, self.image_files[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:18:16.652722Z",
     "iopub.status.busy": "2025-03-10T12:18:16.652422Z",
     "iopub.status.idle": "2025-03-10T12:18:16.661486Z",
     "shell.execute_reply": "2025-03-10T12:18:16.660741Z",
     "shell.execute_reply.started": "2025-03-10T12:18:16.652701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Define the weak and strong augmentations for FixMatch\n",
    "class RandAugment:\n",
    "    \"\"\"RandAugment implementation for strong augmentation in FixMatch\"\"\"\n",
    "    def __init__(self, n=2, m=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n: Number of augmentations to apply\n",
    "            m: Magnitude of the augmentations (0-10)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.augment_list = [\n",
    "            transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "            transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "            transforms.RandomPosterize(bits=4, p=0.5),\n",
    "            transforms.RandomEqualize(p=0.5),\n",
    "            transforms.RandomSolarize(threshold=128, p=0.5)\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        ops = random.choices(self.augment_list, k=self.n)\n",
    "        for op in ops:\n",
    "            img = op(img)\n",
    "        return img\n",
    "\n",
    "# Define the image transformations\n",
    "# Weak augmentation\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Strong augmentation\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    RandAugment(n=2, m=10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transform for validation/test\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:18:34.626708Z",
     "iopub.status.busy": "2025-03-10T12:18:34.626403Z",
     "iopub.status.idle": "2025-03-10T12:19:51.481732Z",
     "shell.execute_reply": "2025-03-10T12:19:51.480848Z",
     "shell.execute_reply.started": "2025-03-10T12:18:34.626683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: Load the datasets and create labeled/unlabeled splits\n",
    "# Load the full training dataset\n",
    "full_train_dataset = ISIC2017Dataset(train_dir, train_gt_dir, transform=None)\n",
    "\n",
    "# Split into labeled and unlabeled data\n",
    "# For FixMatch, we'll use a small portion of labeled data (e.g., 10%)\n",
    "labeled_ratio = 0.1\n",
    "num_train = len(full_train_dataset)\n",
    "num_labeled = int(labeled_ratio * num_train)\n",
    "num_unlabeled = num_train - num_labeled\n",
    "\n",
    "# Generate random indices for the split\n",
    "indices = list(range(num_train))\n",
    "random.shuffle(indices)\n",
    "labeled_indices = indices[:num_labeled]\n",
    "unlabeled_indices = indices[num_labeled:]\n",
    "\n",
    "print(f\"Total training data: {num_train}\")\n",
    "print(f\"Labeled data: {num_labeled} ({labeled_ratio*100:.1f}%)\")\n",
    "print(f\"Unlabeled data: {num_unlabeled} ({(1-labeled_ratio)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:20:08.666605Z",
     "iopub.status.busy": "2025-03-10T12:20:08.666313Z",
     "iopub.status.idle": "2025-03-10T12:21:08.696071Z",
     "shell.execute_reply": "2025-03-10T12:21:08.695310Z",
     "shell.execute_reply.started": "2025-03-10T12:20:08.666585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Create FixMatch-specific dataset classes\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[self.indices[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, dataset, indices, weak_transform=None, strong_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.dataset[self.indices[idx]]\n",
    "        \n",
    "        # Apply both weak and strong augmentations\n",
    "        weak_image = self.weak_transform(image)\n",
    "        strong_image = self.strong_transform(image)\n",
    "        \n",
    "        return weak_image, strong_image\n",
    "\n",
    "# Create the labeled and unlabeled datasets\n",
    "labeled_dataset = LabeledDataset(full_train_dataset, labeled_indices, transform=weak_transform)\n",
    "unlabeled_dataset = UnlabeledDataset(full_train_dataset, unlabeled_indices, \n",
    "                                   weak_transform=weak_transform, \n",
    "                                   strong_transform=strong_transform)\n",
    "\n",
    "# Create the validation and test datasets\n",
    "valid_dataset = ISIC2017Dataset(valid_dir, valid_gt_dir, transform=test_transform)\n",
    "test_dataset = ISIC2017Dataset(test_dir, test_gt_dir, transform=test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:21:35.920801Z",
     "iopub.status.busy": "2025-03-10T12:21:35.920492Z",
     "iopub.status.idle": "2025-03-10T12:21:35.927141Z",
     "shell.execute_reply": "2025-03-10T12:21:35.926089Z",
     "shell.execute_reply.started": "2025-03-10T12:21:35.920779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Create data loaders - Optimized for multi-GPU setup\n",
    "# Increase batch size to utilize multiple GPUs\n",
    "batch_size = 32  # Increased from 16 to better utilize GPUs\n",
    "num_workers = 4  # Increased worker threads for data loading\n",
    "\n",
    "# If using multiple GPUs, scale batch size accordingly\n",
    "if torch.cuda.device_count() > 1:\n",
    "    batch_size *= torch.cuda.device_count()\n",
    "    print(f\"Scaling batch size to {batch_size} for {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "# Pin memory for faster data transfer to GPU\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True, \n",
    "                           num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size*7, shuffle=True, \n",
    "                             num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, \n",
    "                         num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                        num_workers=num_workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:21:51.827807Z",
     "iopub.status.busy": "2025-03-10T12:21:51.827509Z",
     "iopub.status.idle": "2025-03-10T12:21:53.356393Z",
     "shell.execute_reply": "2025-03-10T12:21:53.355475Z",
     "shell.execute_reply.started": "2025-03-10T12:21:51.827784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: Define the model for FixMatch - Support for multi-GPU\n",
    "class FixMatchModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FixMatchModel, self).__init__()\n",
    "        # Use ResNet50 pretrained on ImageNet as the backbone\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "        # Replace the final fully connected layer\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Create the model and move it to the device\n",
    "model = FixMatchModel().to(device)\n",
    "\n",
    "# Enable multi-GPU training with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:23:57.054225Z",
     "iopub.status.busy": "2025-03-10T12:23:57.053879Z",
     "iopub.status.idle": "2025-03-10T12:23:57.065990Z",
     "shell.execute_reply": "2025-03-10T12:23:57.065154Z",
     "shell.execute_reply.started": "2025-03-10T12:23:57.054199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 9: Define the FixMatch training function with GPU optimization\n",
    "def train_fixmatch(model, labeled_loader, unlabeled_loader, optimizer, scheduler, \n",
    "                  num_epochs=100, threshold=0.95, lambda_u=1.0):\n",
    "    \"\"\"\n",
    "    Train using the FixMatch algorithm\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        labeled_loader: DataLoader for labeled data\n",
    "        unlabeled_loader: DataLoader for unlabeled data\n",
    "        optimizer: Optimizer for training\n",
    "        scheduler: Learning rate scheduler\n",
    "        num_epochs: Number of training epochs\n",
    "        threshold: Confidence threshold for pseudo-labeling\n",
    "        lambda_u: Weight for the unsupervised loss\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_acc = 0.0\n",
    "    train_losses, val_accs = [], []\n",
    "    \n",
    "    # Set up mixed precision training for better GPU utilization\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        unlabeled_iter = iter(unlabeled_loader)\n",
    "        \n",
    "        # Clear GPU cache before each epoch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        with tqdm(labeled_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "            for batch_idx, (inputs_x, targets_x) in enumerate(pbar):\n",
    "                try:\n",
    "                    # Get unlabeled batch\n",
    "                    (inputs_u_w, inputs_u_s) = next(unlabeled_iter)\n",
    "                except StopIteration:\n",
    "                    unlabeled_iter = iter(unlabeled_loader)\n",
    "                    (inputs_u_w, inputs_u_s) = next(unlabeled_iter)\n",
    "                \n",
    "                # Move data to device\n",
    "                inputs_x, targets_x = inputs_x.to(device), targets_x.to(device)\n",
    "                inputs_u_w, inputs_u_s = inputs_u_w.to(device), inputs_u_s.to(device)\n",
    "                \n",
    "                batch_size = inputs_x.shape[0]\n",
    "                \n",
    "                # Using mixed precision for faster training on GPU\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Forward pass for labeled data\n",
    "                    outputs_x = model(inputs_x)\n",
    "                    \n",
    "                    # Forward pass for unlabeled data (weak and strong augmentation)\n",
    "                    with torch.no_grad():\n",
    "                        outputs_u_w = model(inputs_u_w)\n",
    "                        # Generate pseudo-labels using the model's predictions on weakly augmented images\n",
    "                        probs_u_w = torch.softmax(outputs_u_w, dim=1)\n",
    "                        max_probs, pseudo_labels = torch.max(probs_u_w, dim=1)\n",
    "                        # Create mask for confident predictions\n",
    "                        mask = max_probs.ge(threshold).float()\n",
    "                    \n",
    "                    # Forward pass for unlabeled data with strong augmentation\n",
    "                    outputs_u_s = model(inputs_u_s)\n",
    "                    \n",
    "                    # Calculate losses\n",
    "                    # Supervised loss on labeled data\n",
    "                    loss_x = criterion(outputs_x, targets_x)\n",
    "                    \n",
    "                    # Unsupervised loss on unlabeled data (only for confident predictions)\n",
    "                    loss_u = torch.mean(\n",
    "                        mask * F.cross_entropy(outputs_u_s, pseudo_labels, reduction='none')\n",
    "                    )\n",
    "                    \n",
    "                    # Combined loss\n",
    "                    loss = loss_x + lambda_u * loss_u\n",
    "                \n",
    "                # Backpropagation with mixed precision\n",
    "                optimizer.zero_grad()\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                # Update progress bar\n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix({\"Loss\": epoch_loss / (batch_idx + 1),\n",
    "                                \"Labeled\": loss_x.item(),\n",
    "                                \"Unlabeled\": loss_u.item(),\n",
    "                                \"Mask\": mask.mean().item()})\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_acc = evaluate(model, valid_loader)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                torch.save(model.module.state_dict(), \"fixmatch_best_model.pth\")\n",
    "            else:\n",
    "                torch.save(model.state_dict(), \"fixmatch_best_model.pth\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss / len(labeled_loader):.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}, Best Val Acc: {best_val_acc:.4f}\")\n",
    "        \n",
    "        # Print GPU memory stats\n",
    "        if torch.cuda.is_available():\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                print(f\"GPU {i} Memory: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB / {torch.cuda.memory_reserved(i) / 1e9:.2f} GB\")\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(labeled_loader))\n",
    "    \n",
    "    # Plot training curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accs)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fixmatch_training_curve.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, val_accs\n",
    "    #no op\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:23:09.015496Z",
     "iopub.status.busy": "2025-03-10T12:23:09.015197Z",
     "iopub.status.idle": "2025-03-10T12:23:09.020886Z",
     "shell.execute_reply": "2025-03-10T12:23:09.019891Z",
     "shell.execute_reply.started": "2025-03-10T12:23:09.015475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 10: Define evaluation function with GPU optimization\n",
    "def evaluate(model, dataloader):\n",
    "    \"\"\"Evaluate the model on the given dataloader\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Use mixed precision for evaluation\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:24:18.591750Z",
     "iopub.status.busy": "2025-03-10T12:24:18.591460Z",
     "iopub.status.idle": "2025-03-10T12:24:18.597003Z",
     "shell.execute_reply": "2025-03-10T12:24:18.596028Z",
     "shell.execute_reply.started": "2025-03-10T12:24:18.591730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 11: Set up optimizer and scheduler - Optimized for GPUs\n",
    "# FixMatch typically uses SGD with momentum and weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "\n",
    "# Learning rate scheduler (cosine annealing)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:24:49.339750Z",
     "iopub.status.busy": "2025-03-10T12:24:49.339436Z",
     "iopub.status.idle": "2025-03-10T12:43:25.755219Z",
     "shell.execute_reply": "2025-03-10T12:43:25.753104Z",
     "shell.execute_reply.started": "2025-03-10T12:24:49.339725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 12: Train the model with FixMatch\n",
    "num_epochs = 100\n",
    "threshold = 0.95  # Confidence threshold for pseudo-labeling\n",
    "lambda_u = 1.0  # Weight for unsupervised loss\n",
    "\n",
    "# Track memory before training\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Before training - GPU {i} Memory: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB / {torch.cuda.memory_reserved(i) / 1e9:.2f} GB\")\n",
    "\n",
    "train_losses, val_accs = train_fixmatch(\n",
    "    model=model,\n",
    "    labeled_loader=labeled_loader,\n",
    "    unlabeled_loader=unlabeled_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=num_epochs,\n",
    "    threshold=threshold,\n",
    "    lambda_u=lambda_u\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 13: Load the best model and evaluate on test set\n",
    "# Clear GPU memory before loading the best model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Load the best model\n",
    "if isinstance(model, nn.DataParallel):\n",
    "    model.module.load_state_dict(torch.load(\"fixmatch_best_model.pth\"))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"fixmatch_best_model.pth\"))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 14: Detailed evaluation with additional metrics\n",
    "def detailed_evaluation(model, dataloader):\n",
    "    \"\"\"Evaluate the model with multiple metrics\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Use mixed precision for faster evaluation\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, average='weighted')\n",
    "    recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets,\n",
    "        'probabilities': all_probs\n",
    "    }\n",
    "\n",
    "# Run detailed evaluation on the test set\n",
    "test_results = detailed_evaluation(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 15: Save the model and results\n",
    "# Save the model\n",
    "if isinstance(model, nn.DataParallel):\n",
    "    model_state_dict = model.module.state_dict()\n",
    "else:\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model_state_dict,\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'val_accuracy': val_accs[-1],\n",
    "    'test_accuracy': test_acc,\n",
    "    'threshold': threshold,\n",
    "    'lambda_u': lambda_u,\n",
    "}, 'fixmatch_isic2017_final.pth')\n",
    "\n",
    "# Save the training history\n",
    "np.savez('fixmatch_training_history.npz', \n",
    "         train_losses=train_losses, \n",
    "         val_accs=val_accs,\n",
    "         test_metrics=test_results)\n",
    "\n",
    "# Print final GPU memory stats\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Final GPU {i} Memory: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB / {torch.cuda.memory_reserved(i) / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"Training complete! Model and results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4953211,
     "sourceId": 8339754,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
