{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1050392,"sourceType":"datasetVersion","datasetId":580960},{"sourceId":2020102,"sourceType":"datasetVersion","datasetId":1209128}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install segmentation_models_pytorch\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchvision.models as models\nfrom torchmetrics.classification import MulticlassJaccardIndex, MulticlassAccuracy, MulticlassConfusionMatrix\nimport timm\nfrom PIL import Image\nfrom tqdm import tqdm\nimport random\nimport glob\nfrom einops import rearrange\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport segmentation_models_pytorch as smp\n\n# Set random seed for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed()\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Mixed precision setup\nuse_amp = True\nscaler = torch.cuda.amp.GradScaler(enabled=use_amp)\nprint(f\"Using mixed precision training (F16+F32): {use_amp}\")\n\n# Constants\nBATCH_SIZE = 32\nIMAGE_HEIGHT = 512\nIMAGE_WIDTH = 1024\nNUM_CLASSES = 19  # Cityscapes has 19 evaluation classes\nNUM_EPOCHS = 20\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-5\nBASE_PATH = '/kaggle/input'\nCITYSCAPES_IMAGES = f'{BASE_PATH}/cityscapes-leftimg8bit-trainvaltest/leftImg8bit'\nCITYSCAPES_MASKS = f'{BASE_PATH}/yrealdataset/gtFine'\n\n# Define Cityscapes color mapping to class indices\n# This is the standard Cityscapes color mapping\ncityscapes_classes = [\n    (0, 0, 0),         # background\n    (128, 64, 128),    # road\n    (244, 35, 232),    # sidewalk\n    (70, 70, 70),      # building\n    (102, 102, 156),   # wall\n    (190, 153, 153),   # fence\n    (153, 153, 153),   # pole\n    (250, 170, 30),    # traffic light\n    (220, 220, 0),     # traffic sign\n    (107, 142, 35),    # vegetation\n    (152, 251, 152),   # terrain\n    (70, 130, 180),    # sky\n    (220, 20, 60),     # person\n    (255, 0, 0),       # rider\n    (0, 0, 142),       # car\n    (0, 0, 70),        # truck\n    (0, 60, 100),      # bus\n    (0, 80, 100),      # train\n    (0, 0, 230),       # motorcycle\n    (119, 11, 32),     # bicycle\n]\n\n# Create a mapping from RGB to class index\ndef create_label_mapping():\n    color_to_class = {}\n    for i, color in enumerate(cityscapes_classes):\n        color_to_class[color] = i\n    return color_to_class\n\ncolor_to_class = create_label_mapping()\n\n# Custom Dataset\nclass CityscapesDataset(Dataset):\n    def __init__(self, split, transform=None):\n        self.split = split\n        self.transform = transform\n        self.color_to_class = color_to_class\n        \n        # Get all image paths\n        self.images = []\n        self.masks = []\n        \n        image_pattern = os.path.join(CITYSCAPES_IMAGES, split, '*', '*_leftImg8bit.png')\n        print(f\"Looking for images in: {image_pattern}\")\n        self.images = sorted(glob.glob(image_pattern))\n        \n        # For each image, find the corresponding mask\n        for img_path in self.images:\n            # Extract city and filename\n            parts = img_path.split('/')\n            city = parts[-2]\n            filename = parts[-1].replace('_leftImg8bit.png', '')\n            \n            # Construct the mask path\n            mask_path = os.path.join(CITYSCAPES_MASKS, split, city, f\"{filename}_gtFine_color.png\")\n            if os.path.exists(mask_path):\n                self.masks.append(mask_path)\n            else:\n                print(f\"Warning: Mask not found for {img_path}\")\n                # Remove the image if mask doesn't exist\n                self.images.remove(img_path)\n        \n        print(f\"Found {len(self.images)} image-mask pairs for {split} set\")\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        mask_path = self.masks[idx]\n        \n        # Load image and mask\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"RGB\"))\n        \n        # Apply transformations if any\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n        \n        # Convert RGB mask to class indices\n        mask_indices = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)\n        \n        # Iterate through each pixel and find the closest class color\n        for i in range(len(cityscapes_classes)):\n            class_color = np.array(cityscapes_classes[i])\n            mask_indices[(np.abs(mask - class_color.reshape(1, 1, 3))).sum(axis=2) < 30] = i\n            \n        return image, mask_indices\n\n# Data Augmentation\ntrain_transform = A.Compose([\n    A.RandomScale(scale_limit=0.1),\n    A.RandomCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\n# Initialize datasets\nprint(\"Initializing datasets...\")\ntrain_dataset = CityscapesDataset(split='train', transform=train_transform)\nval_dataset = CityscapesDataset(split='val', transform=val_transform)\ntest_dataset = CityscapesDataset(split='test', transform=val_transform)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n\n# CNN-ViT Hybrid Model\nclass CNNViTHybrid(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super(CNNViTHybrid, self).__init__()\n        \n        # CNN Backbone (ResNet50 without classification head)\n        self.cnn_backbone = models.resnet50(pretrained=True)\n        self.cnn_features = nn.Sequential(*list(self.cnn_backbone.children())[:-2])\n        \n        # ViT Encoder using timm\n        self.vit = timm.create_model('vit_base_patch16_384', pretrained=True, num_classes=0)\n        vit_embed_dim = self.vit.embed_dim  # Typically 768 for base model\n        \n        # Adapter to match CNN feature dimensions to ViT input\n        self.adapter = nn.Conv2d(2048, 768, kernel_size=1)\n        \n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(768, 512, kernel_size=2, stride=2),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            nn.ConvTranspose2d(64, num_classes, kernel_size=2, stride=2),\n        )\n    \n    def forward(self, x):\n        # Get CNN features\n        cnn_features = self.cnn_features(x)  # [B, 2048, H/32, W/32]\n        print(f\"CNN features shape: {cnn_features.shape}\") if random.random() < 0.01 else None\n        \n        # Adapt CNN features for ViT\n        adapted_features = self.adapter(cnn_features)  # [B, 768, H/32, W/32]\n        \n        # Reshape for ViT\n        B, C, H, W = adapted_features.shape\n        adapted_features = rearrange(adapted_features, 'b c h w -> b (h w) c')\n        \n        # Pass through ViT\n        vit_features = self.vit.forward_features(adapted_features)  # [B, (H/32)*(W/32), 768]\n        print(f\"ViT features shape: {vit_features.shape}\") if random.random() < 0.01 else None\n        \n        # Reshape back to spatial dimensions\n        vit_features = rearrange(vit_features, 'b (h w) c -> b c h w', h=H, w=W)\n        \n        # Decode to segmentation map\n        output = self.decoder(vit_features)  # [B, num_classes, H, W]\n        print(f\"Output shape: {output.shape}\") if random.random() < 0.01 else None\n        \n        return output\n\n# Focal Tversky Loss\nclass FocalTverskyLoss(nn.Module):\n    def __init__(self, alpha=0.5, beta=0.5, gamma=2.0, smooth=1e-5):\n        super(FocalTverskyLoss, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.smooth = smooth\n\n    def forward(self, inputs, targets):\n        # Flatten inputs and targets\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        # True Positives, False Positives & False Negatives\n        TP = (inputs * targets).sum()    \n        FP = ((1-targets) * inputs).sum()\n        FN = (targets * (1-inputs)).sum()\n        \n        # Tversky index\n        tversky = (TP + self.smooth) / (TP + self.alpha*FP + self.beta*FN + self.smooth)  \n        \n        # Focal Tversky loss\n        focal_tversky = (1 - tversky) ** self.gamma\n        \n        return focal_tversky\n\n# Initialize model, loss, and optimizer\nprint(\"Initializing model...\")\nmodel = CNNViTHybrid(num_classes=NUM_CLASSES).to(device)\ncriterion = FocalTverskyLoss(alpha=0.7, beta=0.3, gamma=2.0)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n\n# Initialize metrics\niou_metric = MulticlassJaccardIndex(num_classes=NUM_CLASSES).to(device)\naccuracy_metric = MulticlassAccuracy(num_classes=NUM_CLASSES).to(device)\nconfusion_matrix = MulticlassConfusionMatrix(num_classes=NUM_CLASSES).to(device)\n\n# Training and validation functions\ndef train_one_epoch(model, loader, optimizer, criterion, epoch):\n    model.train()\n    running_loss = 0.0\n    running_iou = 0.0\n    running_acc = 0.0\n    \n    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n    \n    for batch_idx, (images, masks) in enumerate(progress_bar):\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass with mixed precision\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            outputs = model(images)\n            loss = criterion(F.softmax(outputs, dim=1), F.one_hot(masks, NUM_CLASSES).permute(0, 3, 1, 2).float())\n        \n        # Backward and optimize with scaler\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        # Calculate metrics\n        preds = outputs.argmax(dim=1)\n        batch_iou = iou_metric(preds, masks)\n        batch_acc = accuracy_metric(preds, masks)\n        \n        # Update running metrics\n        running_loss += loss.item()\n        running_iou += batch_iou.item()\n        running_acc += batch_acc.item()\n        \n        # Update progress bar\n        progress_bar.set_postfix({\n            'loss': running_loss / (batch_idx + 1),\n            'iou': running_iou / (batch_idx + 1),\n            'acc': running_acc / (batch_idx + 1)\n        })\n        \n        # Print occasional sample metrics\n        if batch_idx % 50 == 0:\n            print(f\"\\nBatch {batch_idx}: Loss: {loss.item():.4f}, IoU: {batch_iou.item():.4f}, Acc: {batch_acc.item():.4f}\")\n    \n    # Calculate epoch metrics\n    epoch_loss = running_loss / len(loader)\n    epoch_iou = running_iou / len(loader)\n    epoch_acc = running_acc / len(loader)\n    \n    return epoch_loss, epoch_iou, epoch_acc\n\ndef validate(model, loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    running_iou = 0.0\n    running_acc = 0.0\n    \n    progress_bar = tqdm(loader, desc=\"Validating\")\n    \n    with torch.no_grad():\n        for batch_idx, (images, masks) in enumerate(progress_bar):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            # Forward pass with mixed precision\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                outputs = model(images)\n                loss = criterion(F.softmax(outputs, dim=1), F.one_hot(masks, NUM_CLASSES).permute(0, 3, 1, 2).float())\n            \n            # Calculate metrics\n            preds = outputs.argmax(dim=1)\n            batch_iou = iou_metric(preds, masks)\n            batch_acc = accuracy_metric(preds, masks)\n            \n            # Update running metrics\n            running_loss += loss.item()\n            running_iou += batch_iou.item()\n            running_acc += batch_acc.item()\n            \n            # Update progress bar\n            progress_bar.set_postfix({\n                'val_loss': running_loss / (batch_idx + 1),\n                'val_iou': running_iou / (batch_idx + 1),\n                'val_acc': running_acc / (batch_idx + 1)\n            })\n    \n    # Calculate epoch metrics\n    epoch_loss = running_loss / len(loader)\n    epoch_iou = running_iou / len(loader)\n    epoch_acc = running_acc / len(loader)\n    \n    return epoch_loss, epoch_iou, epoch_acc\n\n# Function to calculate Dice coefficient\ndef dice_coefficient(pred, target):\n    smooth = 1e-5\n    pred = pred.view(-1)\n    target = target.view(-1)\n    \n    intersection = (pred * target).sum()\n    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n    \n    return dice.item()\n\n# Training loop\nprint(\"Starting training...\")\nhistory = {\n    'train_loss': [],\n    'train_iou': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_iou': [],\n    'val_acc': [],\n    'dice': []\n}\n\nbest_val_iou = 0.0\n\nfor epoch in range(NUM_EPOCHS):\n    # Train\n    train_loss, train_iou, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, epoch)\n    \n    # Validate\n    val_loss, val_iou, val_acc = validate(model, val_loader, criterion)\n    \n    # Calculate Dice on validation\n    model.eval()\n    dice_scores = []\n    with torch.no_grad():\n        for images, masks in tqdm(val_loader, desc=\"Calculating Dice\"):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            with torch.cuda.amp.autocast(enabled=use_amp):\n                outputs = model(images)\n                \n            preds = outputs.argmax(dim=1)\n            \n            # Convert to one-hot for dice calculation\n            one_hot_preds = F.one_hot(preds, NUM_CLASSES).permute(0, 3, 1, 2).float()\n            one_hot_masks = F.one_hot(masks, NUM_CLASSES).permute(0, 3, 1, 2).float()\n            \n            # Calculate dice for each class and average\n            batch_dice = 0\n            for cls in range(NUM_CLASSES):\n                batch_dice += dice_coefficient(one_hot_preds[:, cls], one_hot_masks[:, cls])\n            batch_dice /= NUM_CLASSES\n            \n            dice_scores.append(batch_dice)\n    \n    val_dice = sum(dice_scores) / len(dice_scores)\n    \n    # Update learning rate\n    scheduler.step()\n    \n    # Save history\n    history['train_loss'].append(train_loss)\n    history['train_iou'].append(train_iou)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_iou'].append(val_iou)\n    history['val_acc'].append(val_acc)\n    history['dice'].append(val_dice)\n    \n    # Print epoch summary\n    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}:\")\n    print(f\"Train - Loss: {train_loss:.4f}, IoU: {train_iou:.4f}, Acc: {train_acc:.4f}\")\n    print(f\"Val   - Loss: {val_loss:.4f}, IoU: {val_iou:.4f}, Acc: {val_acc:.4f}, Dice: {val_dice:.4f}\")\n    \n    # Save best model\n    if val_iou > best_val_iou:\n        best_val_iou = val_iou\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(f\"New best model saved with IoU: {best_val_iou:.4f}\")\n\nprint(\"Training complete!\")\n\n# Plot metrics\nplt.figure(figsize=(20, 15))\n\n# Plot Loss\nplt.subplot(2, 2, 1)\nplt.plot(history['train_loss'], label='Train Loss')\nplt.plot(history['val_loss'], label='Val Loss')\nplt.title('Loss Evolution')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\n# Plot IoU\nplt.subplot(2, 2, 2)\nplt.plot(history['train_iou'], label='Train IoU')\nplt.plot(history['val_iou'], label='Val IoU')\nplt.title('IoU Evolution')\nplt.xlabel('Epoch')\nplt.ylabel('IoU')\nplt.legend()\nplt.grid(True)\n\n# Plot Accuracy\nplt.subplot(2, 2, 3)\nplt.plot(history['train_acc'], label='Train Accuracy')\nplt.plot(history['val_acc'], label='Val Accuracy')\nplt.title('Accuracy Evolution')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\n\n# Plot Dice\nplt.subplot(2, 2, 4)\nplt.plot(history['dice'], label='Validation Dice')\nplt.title('Dice Coefficient Evolution')\nplt.xlabel('Epoch')\nplt.ylabel('Dice')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.savefig('training_metrics.png')\nplt.show()\n\n# Load best model for testing\nprint(\"Loading best model for testing...\")\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\n# Test the model and create confusion matrix\nprint(\"Testing model and creating confusion matrix...\")\ntest_iou = 0.0\ntest_acc = 0.0\ntest_dice = 0.0\nall_preds = []\nall_masks = []\n\nwith torch.no_grad():\n    for images, masks in tqdm(test_loader, desc=\"Testing\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        with torch.cuda.amp.autocast(enabled=use_amp):\n            outputs = model(images)\n        \n        preds = outputs.argmax(dim=1)\n        \n        # Update metrics\n        test_iou += iou_metric(preds, masks).item()\n        test_acc += accuracy_metric(preds, masks).item()\n        \n        # Calculate dice\n        one_hot_preds = F.one_hot(preds, NUM_CLASSES).permute(0, 3, 1, 2).float()\n        one_hot_masks = F.one_hot(masks, NUM_CLASSES).permute(0, 3, 1, 2).float()\n        \n        batch_dice = 0\n        for cls in range(NUM_CLASSES):\n            batch_dice += dice_coefficient(one_hot_preds[:, cls], one_hot_masks[:, cls])\n        batch_dice /= NUM_CLASSES\n        \n        test_dice += batch_dice\n        \n        # Collect predictions and masks for confusion matrix\n        all_preds.append(preds.flatten())\n        all_masks.append(masks.flatten())\n        \n        # Update confusion matrix for each batch\n        confusion_matrix.update(preds, masks)\n\n# Calculate final test metrics\ntest_iou /= len(test_loader)\ntest_acc /= len(test_loader)\ntest_dice /= len(test_loader)\n\nprint(f\"\\nTest Results - IoU: {test_iou:.4f}, Acc: {test_acc:.4f}, Dice: {test_dice:.4f}\")\n\n# Get the confusion matrix\nconf_matrix = confusion_matrix.compute().cpu().numpy()\n\n# Plot confusion matrix\nplt.figure(figsize=(12, 10))\nplt.imshow(conf_matrix, cmap='Blues')\nplt.colorbar()\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\n\n# Add text annotations to the confusion matrix\nthresh = conf_matrix.max() / 2\nfor i in range(conf_matrix.shape[0]):\n    for j in range(conf_matrix.shape[1]):\n        plt.text(j, i, f'{int(conf_matrix[i, j])}',\n                 ha=\"center\", va=\"center\",\n                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n\nplt.tight_layout()\nplt.savefig('confusion_matrix.png')\nplt.show()\n\n# Function to visualize predictions\ndef visualize_prediction(model, dataset, idx):\n    image, mask = dataset[idx]\n    image_tensor = image.unsqueeze(0).to(device)\n    mask = mask.numpy()\n    \n    # Get prediction\n    model.eval()\n    with torch.no_grad():\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            output = model(image_tensor)\n        pred = output.argmax(dim=1).squeeze().cpu().numpy()\n    \n    # Convert image back from normalized tensor for visualization\n    image = image.permute(1, 2, 0).cpu().numpy()\n    image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n    image = np.clip(image, 0, 1)\n    \n    # Create color masks\n    mask_colored = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n    pred_colored = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n    \n    for class_idx, color in enumerate(cityscapes_classes):\n        mask_colored[mask == class_idx] = color\n        pred_colored[pred == class_idx] = color\n    \n    # Plot\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    plt.imshow(image)\n    plt.title('Input Image')\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(mask_colored)\n    plt.title('Ground Truth')\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 3)\n    plt.imshow(pred_colored)\n    plt.title('Prediction')\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(f'prediction_example_{idx}.png')\n    plt.show()\n\n# Visualize some predictions\nfor i in [0, 10, 20]:\n    visualize_prediction(model, test_dataset, i)\n\nprint(\"All done! Model has been trained, evaluated, and visualizations have been created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T20:24:53.265486Z","iopub.execute_input":"2025-05-05T20:24:53.266281Z","iopub.status.idle":"2025-05-05T20:26:02.898413Z","shell.execute_reply.started":"2025-05-05T20:24:53.266255Z","shell.execute_reply":"2025-05-05T20:26:02.897314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}